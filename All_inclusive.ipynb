{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pingouin as pg\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PinWei\\my_Haskins_project\\Stats\\NTNU\\Indv_slopes.csv\n"
     ]
    }
   ],
   "source": [
    "F_ID = 9\n",
    "\n",
    "if F_ID == 8:\n",
    "    char_list_path = os.path.join(top_dir, \"Data\", \"NTNU\", \"char_list.txt\")\n",
    "    \n",
    "    with open(char_list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "         char_list = [ char.replace('\\n', '') for char in f.readlines() ]\n",
    "            \n",
    "    fn, dfn = {\n",
    "        0: (\"Data_all.RData\", \"DF\"), \n",
    "        1: (\"Data_all.z.RData\", \"DF.z\"), \n",
    "        2: (\"Data_clean.RData\", \"DF.clean\"), \n",
    "        3: (\"Data_clean.z.RData\", \"DF.clean.z\"), \n",
    "        4: (\"Data_else.RData\", \"DF.else\")\n",
    "    }[1]\n",
    "            \n",
    "else:\n",
    "    fn = \"\"\n",
    "\n",
    "top_dir = os.path.join(\"C:\\\\\", \"Users\", \"PinWei\", \"my_Haskins_project\")\n",
    "\n",
    "data_path = {\n",
    "#     1: os.path.join(top_dir, \"Data\", \"Tse_et_al\", \"Tse_et_al.xlsx\"), \n",
    "#     2: os.path.join(top_dir, \"Data\", \"Tse_et_al\", \"Chan and Tse.xlsx\"), \n",
    "#     3: os.path.join(top_dir, \"Data\", \"HK_norm_2021.xlsx\"), \n",
    "#     4: os.path.join(top_dir, \"Data\", \"LD2T2020_20240904.xlsx\"), \n",
    "#     5: os.path.join(top_dir, \"Data\", \"Tse_et_al\", \"CLP_data.xlsx\"), \n",
    "    6: os.path.join(top_dir, \"Data\", \"Tse_et_al\", [\n",
    "        \"Tse_Yap_Chan_nz.csv\", \"Tse_Yap_Chan.csv\"][1]), \n",
    "    7: os.path.join(top_dir, \"Data\", \"Chang_et_al\", [\n",
    "        \"Chang_Lee_2020.xlsx\", \"Chang_Lee_2020_z.xlsx\"][1]), \n",
    "    8: os.path.join(top_dir, \"Data\", \"NTNU\", fn), \n",
    "    9: os.path.join(top_dir, \"Stats\", \"NTNU\", \"Indv_slopes.csv\")\n",
    "}[F_ID]\n",
    "\n",
    "if F_ID == 4:\n",
    "    data = pd.read_excel(data_path, sheet_name=1)\n",
    "elif F_ID == 6 or F_ID == 9:\n",
    "    data = pd.read_csv(data_path)\n",
    "elif F_ID == 8:\n",
    "    data = pyreadr.read_r(data_path)[dfn]\n",
    "else:\n",
    "    data = pd.read_excel(data_path)\n",
    "\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SID', 'Sex', 'Grade', 'Score', 'logF', 'OPC', 'OSC', 'Imag'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Score', 'logF', 'OPC', 'OSC', 'Imag']\n"
     ]
    }
   ],
   "source": [
    "if F_ID == 5:\n",
    "    cols = [\"NS_1\", \"NS_2\", \"Phon_1\", \"Phon_2\", \n",
    "            \"Freq_C1\", \"Freq_C2\", \"Freq_word\", \n",
    "            \"REG.1_C1\", \"REG.1_C2\", \"REG.2_C1\", \"REG.2_C2\", \n",
    "            \"P.cons_1\", \"P.cons_2\", \"OP.cons_1\", \"OP.cons_2\", \n",
    "            \"HD_1\", \"HD_2\", \"NbS_1\", \"NbS_2\", \n",
    "            \"NMe_1\", \"NMe_2\", \"Trans_1\", \"Trans_2\", \n",
    "            \"Vale\", \"Arou\", \"Conc\", \"Fami\", \"Imag\"]\n",
    "    \n",
    "elif F_ID == 6:\n",
    "    cols = ['C1_Reg.1', 'C1_Reg.2', 'C2_Reg.1', 'C2_Reg.2', \n",
    "            'C1_St.num.Z', 'C2_St.num.Z', \n",
    "            'C1_logF.Z', 'C2_logF.Z', 'Word_logF.Z', # 'C_logF.sum.Z', \n",
    "            'C1_Ph.num.Z', 'C2_Ph.num.Z', \n",
    "            'C1_Homo.D.Z', 'C2_Homo.D.Z',\n",
    "            'C1_Ph.cons.Z', 'C2_Ph.cons.Z', \n",
    "            'C1_OP.cons.Z', 'C2_OP.cons.Z', \n",
    "            'C1_A_Nb.num.Z', 'C2_B_Nb.num.Z', \n",
    "            'C1_A_Nb.logF.Z', 'C2_B_Nb.logF.Z', \n",
    "            'C1_S.num.Z', 'C2_S.num.Z', \n",
    "            'C1_S.Trans.Z', 'C2_S.Trans.Z', # 'C_S.T.avg.Z', \n",
    "            'C1_S.T.Nb.Z', 'C2_S.T.Nb.Z', \n",
    "            'Imag.Z']\n",
    "\n",
    "elif F_ID == 7:\n",
    "    data = data.rename(columns={\"character\": \"Char\"})\n",
    "    cols = list(data.columns[14: ])\n",
    "    \n",
    "elif F_ID == 8:\n",
    "    data = data.rename(columns={\n",
    "        \"OS.Con.type\": \"OSC\", \n",
    "        \"OP.Con.type\": \"OPC\", \n",
    "    })\n",
    "\n",
    "    cols = [\"NS\", \"Freq\", \"logF\", \"HD\", \"P.Reg\", \"P.Unp\", \"OPC\", \n",
    "            \"OSC\", \"SAR\", \"Imag\", \"Conc\", \"Fami\", \"NM\"]\n",
    "\n",
    "#     for sc_add in [\"w2v\", \"GloVe\", \"DSG\"]:\n",
    "#         cols.insert(cols.index(\"OSC\")+1, \"OSC.\" + sc_add)\n",
    "\n",
    "    for col in cols:\n",
    "        data[col] = data[col].astype(float)\n",
    "        \n",
    "elif F_ID == 9:\n",
    "    cols = list(data.columns[3: ])\n",
    "    \n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SID        int64\n",
      "Sex       object\n",
      "Grade      int64\n",
      "Score      int64\n",
      "logF     float64\n",
      "OPC      float64\n",
      "OSC      float64\n",
      "Imag     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save description to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_desc = (data[cols].describe()\n",
    "             .round(3)\n",
    "             .loc[['count', 'max', 'min', 'mean', 'std'], :]\n",
    "             .T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_ID == 8: \n",
    "    desc_fn = (os.path.basename(data_path)\n",
    "               .replace(\"Data_\", \"Desc_\")\n",
    "               .replace(\".RData\", \".xlsx\"))\n",
    "else:\n",
    "    desc_fn = os.path.basename(data_path).split(\".\")[0]\n",
    "\n",
    "desc_path = {\n",
    "    6: os.path.join(top_dir, \"Stats\", \"Tse_et_al\", f\"Desc_{desc_fn}.xlsx\"), \n",
    "    7: os.path.join(top_dir, \"Stats\", \"Chang_et_al\", f\"Desc_{desc_fn}.xlsx\"), \n",
    "    8: os.path.join(top_dir, \"Stats\", \"NTNU\", desc_fn), \n",
    "    9: os.path.join(top_dir, \"Stats\", \"NTNU\", f\"Desc_{desc_fn}.xlsx\")\n",
    "}[F_ID]\n",
    "\n",
    "data_desc.to_excel(desc_path)\n",
    "print(f\"saved: {desc_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_ID == 6: \n",
    "    except_cols = ['C1_Reg.1', 'C1_Reg.2', 'C2_Reg.1', 'C2_Reg.2']\n",
    "    fd = os.path.join(top_dir, \"Figs\", \"Tse_et_al\")\n",
    "\n",
    "elif F_ID == 7: \n",
    "    except_cols = [\"REG\", \"UNP\"]\n",
    "    fd = os.path.join(top_dir, \"Figs\", \"Chang_et_al\")\n",
    "    \n",
    "elif F_ID == 8: \n",
    "    except_cols = [\"P.Reg\", \"P.Unp\"]\n",
    "    fd = os.path.join(top_dir, \"Figs\", \"NTNU\", dfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (F_ID == 6) or if (F_ID == 7) or (F_ID == 8):  \n",
    "    \n",
    "    no_dup_char = data.drop_duplicates(subset = [\"Char\"])\n",
    "    \n",
    "    targ_cols = [ c for c in cols if c not in except_cols ]\n",
    "    \n",
    "    for targ_col in targ_cols:\n",
    "\n",
    "        fn = f\"[histplot] {targ_col}.png\"\n",
    "        if not os.path.exists(fd):\n",
    "            os.makedirs(fd) \n",
    "\n",
    "        sns.histplot(\n",
    "            data=no_dup_char, \n",
    "            x=targ_col, \n",
    "            binrange=(-3, 3), \n",
    "            kde=True, \n",
    "            bins=30\n",
    "        )\n",
    "        plt.tight_layout() \n",
    "        plt.savefig(os.path.join(fd, fn), format='png', dpi=200)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"saved: {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_ID == 8:  \n",
    "    \n",
    "    no_dup_char = data.drop_duplicates(subset = [\"Char\"])\n",
    "    \n",
    "    setA = no_dup_char.query(\"Char in @char_list\")\n",
    "\n",
    "    for targ_col in targ_cols:\n",
    "\n",
    "        fn = f\"[hist_compare] {targ_col}.png\"\n",
    "        if not os.path.exists(fd):\n",
    "            os.makedirs(fd)\n",
    "            \n",
    "        setB = (no_dup_char\n",
    "                .query(\"Char not in @char_list\")\n",
    "                .dropna(subset=[targ_col]))\n",
    "\n",
    "        plt.style.use('seaborn-v0_8-white')\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.hist([setA[targ_col], setB[targ_col]], \n",
    "                bins=30, \n",
    "                range=[-3, 3], \n",
    "                color=['b', 'orange'], \n",
    "                alpha=0.8, \n",
    "                label=[f\"{len(setA)} char\", f\"{len(setB)} char\"])\n",
    "        ax.set_xlabel(targ_col, fontsize=16)\n",
    "        ax.set_ylabel(\"Count\", fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.legend(loc='upper right', fontsize=14)\n",
    "        plt.tight_layout() \n",
    "        plt.savefig(os.path.join(fd, fn), format='png', dpi=200)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"saved: {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot correlation matrix and save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PinWei\\my_Haskins_project\\Figs\\Chang_et_al\\[cormat] Chang_Lee_2020_z.png\n"
     ]
    }
   ],
   "source": [
    "if F_ID == 8: \n",
    "    desc_fn = (os.path.basename(data_path)\n",
    "               .replace(\"Data_\", \"[cormat] \")\n",
    "               .replace(\".RData\", \".png\"))\n",
    "else:\n",
    "    desc_fn = os.path.basename(data_path).split(\".\")[0]\n",
    "\n",
    "cormat_path = {\n",
    "    6: os.path.join(top_dir, \"Figs\", \"Tse_et_al\", f\"[cormat] {desc_fn}.png\"), \n",
    "    7: os.path.join(top_dir, \"Figs\", \"Chang_et_al\", f\"[cormat] {desc_fn}.png\"), \n",
    "    8: os.path.join(top_dir, \"Figs\", \"NTNU\", desc_fn), \n",
    "    9: os.path.join(top_dir, \"Figs\", \"NTNU\", f\"Slope cormat.png\")\n",
    "}[F_ID]\n",
    "\n",
    "print(cormat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_SIG = 0\n",
    "\n",
    "# ---\n",
    "\n",
    "df_select = data.loc[:, cols]\n",
    "r_raw = df_select.corr()\n",
    "p_raw = df_select.corr(method=lambda x, y: stats.pearsonr(x, y)[1]) - np.eye(*r_raw.shape)\n",
    "\n",
    "if PLOT_SIG:\n",
    "    n_of_comp = np.sum([ n for n in range(len(cols)) ])\n",
    "    corrected_thres = [ alpha/n_of_comp for alpha in [.05, .01, .001] ]\n",
    "    p_sig = p_raw.applymap(lambda x: \"\".join(['*' for thres in corrected_thres if x <= thres]))\n",
    "    r_sig = r_raw.round(2).astype(str) + p_sig\n",
    "else:\n",
    "    r_sig = r_raw.round(2).astype(str)\n",
    "\n",
    "# print(round(p_raw, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\Chang_et_al\\[cormat] Chang_Lee_2020_z.png\n"
     ]
    }
   ],
   "source": [
    "mask = np.zeros_like(r_raw)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "if F_ID == 7: \n",
    "    fig, ax = plt.subplots(figsize=(8, 6), dpi=200)\n",
    "    fz, xr = 16, 0\n",
    "    \n",
    "elif F_ID == 8: \n",
    "    fig, ax = plt.subplots(figsize=(12, 8), dpi=200)\n",
    "    fz, xr = 16, 0\n",
    "    \n",
    "elif F_ID == 9: \n",
    "    fig, ax = plt.subplots(figsize=(5, 3), dpi=200)\n",
    "    fz, xr = 12, 0\n",
    "\n",
    "sns.heatmap(\n",
    "    data=r_raw, mask=mask, \n",
    "    square=False, vmin=-1, vmax=1, \n",
    "    cmap=\"RdBu_r\", cbar=False, \n",
    "    annot=r_sig, fmt=\"\", annot_kws={'size': fz}, \n",
    "    linewidth=.5\n",
    ")\n",
    "ax.set_xticklabels(cols, rotation=xr, fontsize=fz)\n",
    "ax.set_yticklabels(cols, rotation=0, fontsize=fz)\n",
    "plt.tight_layout() \n",
    "plt.savefig(cormat_path, format='png')\n",
    "plt.close()\n",
    "\n",
    "print(f\"saved: {cormat_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_ID == 9: \n",
    "    \n",
    "    out_path = os.path.join(\n",
    "        top_dir, \"Stats\", \"NTNU\", \"Indv_slopes_corr.csv\")\n",
    "\n",
    "    cols = data.columns[3:]\n",
    "    \n",
    "    corrs = {}\n",
    "    for gd in range(1, 7):\n",
    "        df_select = data.query(\"Grade == @gd\").loc[:, cols]\n",
    "        corr_matrix = df_select.corr().to_numpy()\n",
    "        corr_vector = corr_matrix[np.triu_indices_from(corr_matrix, k=1)]\n",
    "        corrs[f\"G{gd}\"] = corr_vector\n",
    "\n",
    "    corr_names = []\n",
    "    for x, c1 in enumerate(cols):\n",
    "        for c2 in cols[x+1::]:\n",
    "            corr_names.append(f\"{c1} × {c2}\")\n",
    "\n",
    "    slopes_corr = pd.DataFrame(corrs, index = corr_names)\n",
    "    slopes_corr.to_csv(out_path)\n",
    "    \n",
    "    print(f\"saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_ID == 9: \n",
    "    \n",
    "    for gd in range(1, 7):\n",
    "        out_path = os.path.join(top_dir, \"Figs\", \"NTNU\", \n",
    "                                f\"slope cormat (G-{gd}).png\") \n",
    "\n",
    "        df_select = data.query(\"Grade == @gd\").loc[:, cols]\n",
    "\n",
    "        r_raw = df_select.corr()\n",
    "        p_raw = df_select.corr(method=lambda x, y: stats.pearsonr(x, y)[1]) - np.eye(*r_raw.shape)\n",
    "        p_sig = p_raw.applymap(lambda x: \"\".join(['*' for thres in corrected_thres if x <= thres]))\n",
    "        r_sig = r_raw.round(2).astype(str) + p_sig\n",
    "\n",
    "        mask = np.zeros_like(r_raw)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 3), dpi=200)\n",
    "        fz = 12 \n",
    "        sns.heatmap(\n",
    "            data = r_raw, mask=mask, \n",
    "            square=False, vmin=-1, vmax=1, \n",
    "            cmap=\"RdBu_r\", cbar=False, \n",
    "            annot=r_sig, fmt=\"\", linewidth=.5\n",
    "        )\n",
    "        ax.set_xticklabels(cols, rotation=0, fontsize=fz)\n",
    "        ax.set_yticklabels(cols, rotation=0, fontsize=fz)\n",
    "        \n",
    "        plt.tight_layout() \n",
    "        plt.savefig(out_path, format='png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line plot (show change across grades):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_ID == 9: \n",
    "    \n",
    "    out_path = os.path.join(\n",
    "        top_dir, \"Figs\", \"NTNU\", \"Slope corrs change across grades.png\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    df = slopes_corr.iloc[:4, :]\n",
    "    for idx, row in df.iterrows():\n",
    "        ax1.plot(df.columns, row, \n",
    "                 marker='o', label=idx)\n",
    "    ax1.grid(True)\n",
    "    ax1.set_ylabel('Correlation coefficient', size=16)\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    df = slopes_corr.loc[[\"OPC × Imag\", \"OPC × OSC\"], :]\n",
    "    for idx, row in df.iterrows():\n",
    "        ax2.plot(df.columns, row, \n",
    "                 marker='o', label=idx)\n",
    "    ax2.grid(True)\n",
    "    ax2.set_xlabel('Grades', size=16)\n",
    "    ax2.set_ylabel('Correlation coefficient', size=16)\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(out_path, format='png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_x = [\"Imag\", \"OSC\"][0]\n",
    "# var_y = \"OPC\"\n",
    "\n",
    "# r, p = stats.pearsonr(data[var_x], data[var_y])\n",
    "# print(f\"Correlation between {var_x} and {var_y} is {r:.2f} (p = {p:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_ID == 9: \n",
    "    \n",
    "    c_min = data[\"Score\"].min()\n",
    "    c_max = data[\"Score\"].max()\n",
    "    \n",
    "    for var_x, x_name in [(\"Imag\", \"Imageability\"), (\"OSC\", \"OSC\")]:\n",
    "        var_y, y_name = (\"OPC\", \"OPC\")\n",
    "\n",
    "        out_path = os.path.join(top_dir, \"Figs\", \"NTNU\", \n",
    "                                f\"[scatter] {var_x} × {var_y}.png\")\n",
    "\n",
    "        data[\"Grade\"] = data[\"Grade\"].astype('category')\n",
    "        markers = ['o', 's', 'D', '^', 'v', 'P']\n",
    "\n",
    "        X = data[var_x].values.reshape(-1, 1)\n",
    "        y = data[var_y].values\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "\n",
    "        y_pred = reg.predict(X)\n",
    "        slope = reg.coef_[0]\n",
    "        intercept = reg.intercept_\n",
    "        equation = f\"y = {slope:.2f}x + {intercept:.2f}\"\n",
    "\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "        sns.scatterplot(\n",
    "            data=data, \n",
    "            ax=ax, \n",
    "            x=var_x, y=var_y, \n",
    "            hue=\"Score\", style=\"Grade\", \n",
    "            hue_norm=(c_min, c_max), \n",
    "            markers=markers, \n",
    "            palette='viridis', \n",
    "            legend=False, \n",
    "            alpha=.7, \n",
    "            s=100\n",
    "        )\n",
    "        ax.plot(\n",
    "            data[var_x], y_pred, \n",
    "            color='black', linestyle='--', linewidth=1\n",
    "        )\n",
    "        ax.text(\n",
    "            0.5, 0.9, equation, \n",
    "            horizontalalignment='center', \n",
    "            verticalalignment='center', \n",
    "            transform=plt.gca().transAxes, \n",
    "            fontsize=16, color='black'\n",
    "        )\n",
    "\n",
    "        norm = Normalize(vmin=c_min, vmax=c_max)\n",
    "        sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=ax)\n",
    "        cbar.set_label(\"Naming score\")\n",
    "\n",
    "        unique_grades = data[\"Grade\"].unique()\n",
    "        grade_handles = [ plt.Line2D([0], [0], marker=markers[i], \n",
    "                                     color='w', markerfacecolor='gray', markersize=10) \n",
    "                          for i, grade in enumerate(unique_grades) ]\n",
    "        ax.legend(grade_handles, unique_grades, title='Grade', fontsize=14)\n",
    "\n",
    "        ax.set_xlabel(f\"Sensivity to {x_name}\", fontsize=16)\n",
    "        ax.set_ylabel(f\"Sensivity to {y_name}\", fontsize=16)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "        plt.tight_layout() \n",
    "        plt.savefig(out_path, dpi=300, bbox_inches='tight') \n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 101.0] Imag × OPC (G-1).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 116.0] Imag × OPC (G-2).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 123.0] Imag × OPC (G-3).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 134.0] Imag × OPC (G-4).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 151.0] Imag × OPC (G-5).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 104.0] Imag × OPC (G-6).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 101.0] OSC × OPC (G-1).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 116.0] OSC × OPC (G-2).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 123.0] OSC × OPC (G-3).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 134.0] OSC × OPC (G-4).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 151.0] OSC × OPC (G-5).png\n",
      "saved: C:\\Users\\PinWei\\my_Haskins_project\\Figs\\NTNU\\[1.0 ~ 104.0] OSC × OPC (G-6).png\n"
     ]
    }
   ],
   "source": [
    "scale = [\"fixed\", \"flexible\", \"ranked\"][-1]\n",
    "\n",
    "if F_ID == 9: \n",
    "    \n",
    "    hue_from = \"Score\"\n",
    "    \n",
    "    if scale == \"fixed\":\n",
    "        c_min = data[\"Score\"].min() \n",
    "        c_max = data[\"Score\"].max()\n",
    "        \n",
    "    for var_x, x_name in [(\"Imag\", \"Imageability\"), (\"OSC\", \"OSC\")]:\n",
    "        var_y, y_name = (\"OPC\", \"OPC\")\n",
    "\n",
    "        for gd in range(1, 7):\n",
    "\n",
    "            df_select = data.query(\"Grade == @gd\").loc[:, cols]\n",
    "            \n",
    "            if scale == \"ranked\":\n",
    "                df_select[\"Score_Rank\"] = df_select[\"Score\"].rank(axis=0)\n",
    "                c_min = df_select[\"Score_Rank\"].min()\n",
    "                c_max = df_select[\"Score_Rank\"].max()\n",
    "                hue_from = \"Score_Rank\"\n",
    "                \n",
    "            elif scale == \"flexible\":\n",
    "                c_min = df_select[\"Score\"].min()\n",
    "                c_max = df_select[\"Score\"].max()\n",
    "\n",
    "            fig_name = f\"[{c_min} ~ {c_max}] {var_x} × {var_y} (G-{gd}).png\"\n",
    "            # fig_name = f\"[scatter] {var_x} × {var_y} (G-{gd}).png\"\n",
    "            \n",
    "            out_path = os.path.join(top_dir, \"Figs\", \"NTNU\", fig_name)\n",
    "\n",
    "            X = df_select[var_x].values.reshape(-1, 1)\n",
    "            y = df_select[var_y].values\n",
    "            reg = LinearRegression().fit(X, y)\n",
    "\n",
    "            y_pred = reg.predict(X)\n",
    "            slope = reg.coef_[0]\n",
    "            intercept = reg.intercept_\n",
    "            equation = f\"y = {slope:.2f}x + {intercept:.2f}\"\n",
    "\n",
    "            sns.set(style=\"whitegrid\")\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "            sns.scatterplot(\n",
    "                data=df_select, \n",
    "                ax=ax, \n",
    "                x=var_x, y=var_y, \n",
    "                hue=hue_from, \n",
    "                hue_norm=(c_min, c_max), \n",
    "                marker='o', \n",
    "                palette='viridis', \n",
    "                legend=False, \n",
    "                alpha=.7, \n",
    "                s=100\n",
    "            )\n",
    "            ax.plot(\n",
    "                df_select[var_x], y_pred, \n",
    "                color='black', linestyle='-', linewidth=1\n",
    "            )\n",
    "            ax.text(\n",
    "                0.5, 0.9, equation, \n",
    "                horizontalalignment='center', \n",
    "                verticalalignment='center', \n",
    "                transform=plt.gca().transAxes, \n",
    "                fontsize=16, \n",
    "                color='black'\n",
    "            )\n",
    "            ax.axvline(\n",
    "                x=0, color=\"darkgray\", linewidth=2, linestyle=\"--\"\n",
    "            )\n",
    "            ax.axhline(\n",
    "                y=0, color=\"darkgray\", linewidth=2, linestyle=\"--\"\n",
    "            )\n",
    "            \n",
    "            norm = Normalize(vmin=c_min, vmax=c_max)\n",
    "            sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
    "            sm.set_array([])\n",
    "            cbar = fig.colorbar(sm, ax=ax)\n",
    "            cbar.set_label(\"Naming score\")\n",
    "\n",
    "            ax.set_xlabel(f\"Sensivity to {x_name}\", fontsize=16)\n",
    "            ax.set_ylabel(f\"Sensivity to {y_name}\", fontsize=16)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "            plt.tight_layout() \n",
    "            plt.savefig(out_path, dpi=300, bbox_inches='tight') \n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_y = data[[\"Acc\", \"zRT\"][1]]\n",
    "# data_x = {\n",
    "#     0: data.iloc[:, 9:],\n",
    "#     1: data.iloc[:, 9:22],                          # phoneme features\n",
    "#     2: data.iloc[:, [22, 23, 26, 27, 28]],          # orthographic features\n",
    "#     3: data.iloc[:, [24, 25]+list(range(29, 39))],  # phonological features\n",
    "#     4: data.iloc[:, -6:]                            # semantic features\n",
    "# }[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_x = sm.add_constant(data_x)\n",
    "# results = sm.OLS(data_y, data_x).fit()\n",
    "# # results = sm.GLM(data_y, data_x).fit()\n",
    "# results.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
